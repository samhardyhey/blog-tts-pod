{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP cost?\n",
    "def string_size_in_bytes(s):\n",
    "    return len(s.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# 23 million bytes\n",
    "(df.article.apply(string_size_in_bytes).sum() - 1000000) * 0.000016\n",
    "# ~353.800848 US for all text, less free month allowance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coqui price estimates?\n",
    "from pathlib import Path\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# 10 TTS\n",
    "audio_files = list(Path(\"./data/tts_output/tacotron2_ddc_ph\").rglob(\"*/*.mp3\"))\n",
    "\n",
    "\n",
    "def get_audio_length(filename):\n",
    "    audio = AudioSegment.from_file(filename)\n",
    "    return len(audio)  # length in milliseconds\n",
    "\n",
    "\n",
    "audio_all = []\n",
    "for audio_file in audio_files:\n",
    "    length = get_audio_length(audio_file) / 1000.0  # convert to seconds\n",
    "    audio_all.append(length)\n",
    "\n",
    "# hours > https://coqui.ai/pricing > $20/4 hours\n",
    "sum(audio_all) / 60 / 60\n",
    "# 2.2028605555555556, yikes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coqui model revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/en/vctk/vits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker in speakers[1:]:\n",
    "    speaker\n",
    "    output_file = output_dir / f\"vctk_{speaker}_nautilus_editors_note.mp3\"\n",
    "    # tts_coqui_vctk(speaker, nautilus_editors_note, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import nautilus_editors_note\n",
    "\n",
    "nautilus_editors_note\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code equivlanet for these models?\n",
    "!tts --text \"Behold the humble nautilus. Just about a foot in diameter, it is a slow bottom-dweller with short tentacles that moves through the water with an unsteady wobble. It\\'s also 500 million years old and, in its day, was the best and brightest, using its newly evolved depth control to lay waste to acre after acre of scuttling crustacean prey.\\nWe became interested in it here at Nautilus because, well, we stole its name. But also because (for a mollusk) it represents a remarkable intersection of science, math, myth, and culture. Since that is exactly the kind of intersection we love to write about, we decided to put together a little \"teaser\" issue all about it.\\nThere\\'s the science. The nautilus has a beautiful, logarithmic, and fractal spiral in its shell. Benoit Mandelbrot, discoverer of the fractal, gives us a few words on that topic. One of the world\\'s foremost nautilus experts, Peter Ward, tells us about nautilus evolution and biology, and about his life of nautilus research.\\nThen, the myth: from Jules Verne\\'s fictional submarine, to Oliver Wendell Holmes\\' poem, to how and why we turn science into story.\\nTwo chapters, one undersea creature. Welcome aboard.\\nMichael Segal\\n    Editor in Chief\"\\\n",
    "--out_path spkr-out.wav --model_name \"tts_models/en/vctk/vits\" \\\n",
    "--use_cuda True \\\n",
    "--speaker_idx \"p227\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tortoise Diffusion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from tortoise.api import MODELS_DIR, TextToSpeech\n",
    "from tortoise.utils.audio import load_voices\n",
    "\n",
    "sys.path.append(\"../tortoise-tts/tortoise\")\n",
    "\n",
    "\n",
    "voices = \"\"\"angie                daniel  freeman  jlaw  myself  rainbow       tom           train_dotrice  train_grace     train_mouse\n",
    "applejack            deniro  geralt   lj    pat     snakes        train_atkins  train_dreams   train_kennard   weaver\n",
    "cond_latent_example  emma    halle    mol   pat2    tim_reynolds  train_daws    train_empire   train_lescault  william\"\"\"\n",
    "\n",
    "\n",
    "output_dir = Path(\"./data/tts_output/dev/tortoise\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "voices_formatted = [word for line in voices.splitlines() for word in line.split()]\n",
    "model = voices_formatted[0]\n",
    "\n",
    "tts = TextToSpeech(models_dir=MODELS_DIR)\n",
    "voice_samples, conditioning_latents = load_voices([model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from utils import nautilus_editors_note\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "sents = [e.text for e in list(nlp(nautilus_editors_note).sents)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sent in enumerate(sents):\n",
    "    print(sent)\n",
    "    gen, dbg_state = tts.tts_with_preset(\n",
    "        sent,\n",
    "        k=1,\n",
    "        voice_samples=voice_samples,\n",
    "        conditioning_latents=conditioning_latents,\n",
    "        preset=\"fast\",\n",
    "        use_deterministic_seed=42,\n",
    "        return_deterministic_state=True,\n",
    "        cvvp_amount=0.0,\n",
    "    )\n",
    "    torchaudio.save(str(output_dir / f\"test_{idx}.wav\"), gen.squeeze(0).cpu(), 24000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils import to_snake_case\n",
    "\n",
    "# re-assign issue/article numbers, whoops\n",
    "df = (\n",
    "    pd.read_csv(\"./data/naut_all.csv\")\n",
    "    .assign(issue_number=lambda x: x.issue_title.factorize()[0] + 1)\n",
    "    .assign(article_number=lambda x: x.groupby(\"issue_number\").cumcount() + 1)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv(\"./data/tts_logs.csv\")\n",
    "\n",
    "median_token_conversion = (dff.time_elapsed / dff.n_tokens).median()\n",
    "median_token_conversion  # median time-per-token 0.007610222857264401 per token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((dff.assign(token_estimate=lambda x: x.n_tokens * median_token_conversion)))\n",
    "\n",
    "#    issue_number  article_number  n_tokens  time_elapsed  token_estimate\n",
    "# 0             1               1       199      7.813745        1.514434\n",
    "# 1             1               2      3129     29.584105       23.812387\n",
    "# 2             1               3      4239     31.648683       32.259735\n",
    "# 3             1               4      2033     14.212531       15.471583\n",
    "# 4             1               5      1928     14.536405       14.672510\n",
    "# 5             1               6      2662     18.884738       20.258413\n",
    "# 6             2               1       355      5.581702        2.701629\n",
    "# 7             2               2      1478     11.352247       11.247909\n",
    "# 8             2               3      3912     29.144785       29.771192\n",
    "# 9             2               4       654      6.788820        4.977086\n",
    "\n",
    "\n",
    "(\n",
    "    df.article.apply(lambda x: len(x.split(\" \"))) * median_token_conversion\n",
    ").sum() / 60 / 60\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 streaming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ingest all articles\n",
    "df = (\n",
    "    pd.read_csv(\"./data/naut_all.csv\")\n",
    "    .assign(issue_number=lambda x: x.issue_title.factorize()[0] + 1)\n",
    "    .assign(article_number=lambda x: x.groupby(\"issue_number\").cumcount() + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "\n",
    "# Extract the S3 bucket and key from the S3 path\n",
    "s3_path = f\"s3://{BUCKET_NAME}/{dff.iloc[0].audio_url}\"\n",
    "bucket = s3_path.split(\"//\")[1].split(\"/\")[0]\n",
    "key = \"/\".join(s3_path.split(\"//\")[1].split(\"/\")[1:])\n",
    "\n",
    "# Generate a valid URL for redirection\n",
    "redirect_url = f\"https://{bucket}.s3.amazonaws.com/{quote(key)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'blog-tts-pod'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# These define the bucket and object to read\n",
    "BUCKET_NAME = \"blog-tts-pod\"\n",
    "NAUT_ALL_OBJECT_KEY = \"data/naut_all.csv\"\n",
    "\n",
    "# Use the S3 client to read the file\n",
    "object_data = s3.get_object(Bucket=BUCKET_NAME, Key=NAUT_ALL_OBJECT_KEY)\n",
    "file_data = object_data['Body'].read()\n",
    "\n",
    "# Convert the file data to a pandas DataFrame\n",
    "data = pd.read_csv(StringIO(file_data.decode('utf-8')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0fb7ad1863bba2e2a149175e2f293cc7effc6691cc580cfc80ad00772d007246"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
